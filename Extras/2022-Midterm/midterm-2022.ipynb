{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Distributed-Information-Systems\" data-toc-modified-id=\"Distributed-Information-Systems-0\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Distributed Information Systems</a></span></li><li><span><a href=\"#Word-Representation-for-Concept-Identification\" data-toc-modified-id=\"Word-Representation-for-Concept-Identification-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Word Representation for Concept Identification</a></span><ul class=\"toc-item\"><li><span><a href=\"#Build-the-vocabulary-by-selecting-top-k-frequent-words\" data-toc-modified-id=\"Build-the-vocabulary-by-selecting-top-k-frequent-words-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>Build the vocabulary by selecting top-k frequent words</a></span></li><li><span><a href=\"#Construct-the-word-cooccurence-matrix\" data-toc-modified-id=\"Construct-the-word-cooccurence-matrix-2.2\"><span class=\"toc-item-num\">2.2&nbsp;&nbsp;</span>Construct the word cooccurence matrix</a></span></li><li><span><a href=\"#Perform-SVD-on-the-matrix-and-select-the-largest-singular-values\" data-toc-modified-id=\"Perform-SVD-on-the-matrix-and-select-the-largest-singular-values-2.3\"><span class=\"toc-item-num\">2.3&nbsp;&nbsp;</span>Perform SVD on the matrix and select the largest singular values</a></span></li></ul></li><li><span><a href=\"#Vector-based-retrieval-using-Word-representations\" data-toc-modified-id=\"Vector-based-retrieval-using-Word-representations-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Vector-based retrieval using Word representations</a></span><ul class=\"toc-item\"><li><span><a href=\"#Document-and-query-vectors-from-word-representations\" data-toc-modified-id=\"Document-and-query-vectors-from-word-representations-3.1\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;</span>Document and query vectors from word representations</a></span></li><li><span><a href=\"#Retrieve-top-10-relevant-documents\" data-toc-modified-id=\"Retrieve-top-10-relevant-documents-3.2\"><span class=\"toc-item-num\">3.2&nbsp;&nbsp;</span>Retrieve top-10 relevant documents</a></span></li></ul></li><li><span><a href=\"#Evaluation\" data-toc-modified-id=\"Evaluation-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Evaluation</a></span><ul class=\"toc-item\"><li><span><a href=\"#Evaluate-retrieval-result-using-DCG\" data-toc-modified-id=\"Evaluate-retrieval-result-using-DCG-4.1\"><span class=\"toc-item-num\">4.1&nbsp;&nbsp;</span>Evaluate retrieval result using DCG</a></span></li><li><span><a href=\"#Explain-the-DCG-values-plot\" data-toc-modified-id=\"Explain-the-DCG-values-plot-4.2\"><span class=\"toc-item-num\">4.2&nbsp;&nbsp;</span>Explain the DCG values plot</a></span></li></ul></li><li><span><a href=\"#Submit-your-notebook\" data-toc-modified-id=\"Submit-your-notebook-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Submit your notebook</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Distributed Information Systems\n",
    "***Midterm Exam, Fall-Winter Semester 2021-22***\n",
    "\n",
    "The following materials are allowed: exercise sheets and solutions, past exams with your own solution, personally written notes and personally collected documentation.\n",
    "\n",
    "The exam will be held on your computer, but digital communication by any means is strictly prohibited. \n",
    "By participating to this exam you agree to these conditions.\n",
    "\n",
    "These are the instructions for the exam:\n",
    "\n",
    "1. You are not allowed to leave the examination room in the first 20 and the last 15 minutes of the exam.\n",
    "* We will publish 15 minutes before the end of the exam a password for uploading your solutions on Moodle.\n",
    "* It is not recommended to leave the exam before the password is published. If you need to leave earlier, contact us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\Mao\n",
      "[nltk_data]     Zhipeng\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to C:\\Users\\Mao\n",
      "[nltk_data]     Zhipeng\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#Required libraries\n",
    "import math\n",
    "import os\n",
    "import string\n",
    "from collections import defaultdict\n",
    "from operator import itemgetter\n",
    "from pprint import pprint\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "import numpy as np\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from nltk.tokenize import TweetTokenizer, word_tokenize\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import linear_kernel\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english')).union(set(stopwords.words('french')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_corpus():\n",
    "    '''Reads corpus from files.'''\n",
    "    \n",
    "    documents = []\n",
    "    orig_docs = []\n",
    "    DIR = './'\n",
    "    tknzr = TweetTokenizer()\n",
    "    with open(\"epfldocs.txt\", encoding = \"utf-8\") as f:\n",
    "        content = f.readlines()\n",
    "    for text in content:\n",
    "        orig_docs.append(text)\n",
    "        # split into words\n",
    "        tokens = tknzr.tokenize(text)\n",
    "        # convert to lower case\n",
    "        tokens = [w.lower() for w in tokens]\n",
    "        # remove punctuation from each word\n",
    "        table = str.maketrans('', '', string.punctuation)\n",
    "        stripped = [w.translate(table) for w in tokens]\n",
    "        # remove remaining tokens that are not alphabetic\n",
    "        words = [word for word in stripped if word.isalpha()]\n",
    "        # filter out stop words\n",
    "        words = [w for w in words if not w in stop_words]\n",
    "\n",
    "        documents.append(' '.join(words))\n",
    "    return documents, orig_docs\n",
    "\n",
    "documents, orig_docs = read_corpus()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert(len(documents) == 1075)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Representation for Concept Identification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will build word representations in a latent concept space using SVD. Differently to Latent Semantic Indexing (LSI) we will derive the latent concepts space from the **word co-occurrence matrix** (and not from the term-document matrix, as in standard LSI).\n",
    "\n",
    "An entry (i,j) in the word co-occurrence matrix corresponds to the number of times the word i co-occurs with the word j in the context of word i. The context of the words consist of the words preceding or succeeding the word in the text.  \n",
    "\n",
    "By deriving an SVD from the word co-occurrence matrix, and selecting the top dimensions of the latent space, we obtain a word representation as vectors over a concept space. Commonly such word representations are also called word embeddings.\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the vocabulary by selecting top-k frequent words\n",
    "No code is required for this part."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vocabulary is the list of all words\n",
    "# vocabulary_to_index maps words to their index\n",
    "\n",
    "def create_vocabulary_frequency(corpus, vocab_len):\n",
    "    '''Select top-k (k = vocab_len) words in term of frequencies as vocabulary'''\n",
    "    vocabulary_to_index = {}\n",
    "    count = defaultdict(int)\n",
    "    for document in corpus:\n",
    "        for word in document.split():\n",
    "                count[word] += 1\n",
    "    \n",
    "    sorted_count_by_freq = sorted(count.items(), key=lambda kv: kv[1], reverse=True)\n",
    "\n",
    "    vocabulary = []\n",
    "    for i, x in enumerate(sorted_count_by_freq[:vocab_len]):\n",
    "        vocabulary.append(x[0])\n",
    "        vocabulary_to_index[x[0]] = i\n",
    "    return vocabulary, vocabulary_to_index\n",
    "\n",
    "vocab_freq, vocabulary_to_index = create_vocabulary_frequency(documents, 5000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construct the word cooccurence matrix\n",
    "\n",
    "In this question, you need to construct the word co-occurence matrix, given the vocabulary and the set of documents.\n",
    "\n",
    "The value of a cell (i,j) is the number of times the word i co-occurs with the word j in the context of word i.\n",
    "\n",
    "For this question, a word $w_i$ cooccurs with a word $w_j$ in the context of word $w_i$ if $w_j$ preceeds or succeeds $w_i$ with a distance **at most 2**.\n",
    "\n",
    "Example: For this document \"*how to bake bread without bake recip*\", the words coocur with the word \"*bread*\" are \"*to, bake, without, bake*\".\n",
    "\n",
    "Make sure that you consider only words that appear in the vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "25000000"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def construct_word_cooccurence_matrix(vocabulary_to_index, documents, k=2):\n",
    "    matrix = np.zeros((len(vocabulary_to_index), len(vocabulary_to_index)))\n",
    "    for document in documents:\n",
    "        terms = document.split()\n",
    "        for ind, term_i in enumerate(terms):\n",
    "            if term_i in vocabulary_to_index:\n",
    "                ww_i = vocabulary_to_index[term_i]\n",
    "                for i in range(max(0, ind - 2), min(len(terms), ind + 3)):\n",
    "                    if i != ind:\n",
    "                        cw = terms[i]\n",
    "                        if cw in vocabulary_to_index:\n",
    "                            cw_i = vocabulary_to_index[cw]\n",
    "                            matrix[ww_i][cw_i] += 1\n",
    "            else:\n",
    "                continue\n",
    "    return matrix\n",
    "\n",
    "word_cooccur_matrix = construct_word_cooccurence_matrix(vocabulary_to_index, documents)\n",
    "\n",
    "print(len(word_cooccur_matrix))\n",
    "np.array(word_cooccur_matrix == word_cooccur_matrix.T).sum()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can optionally check whether the matrix you constructed is correct using the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert_matrix = True\n",
    "if assert_matrix:\n",
    "    word_coor_mat = np.load(\"word_coocur_matrix.npy\")\n",
    "    assert(word_coor_mat == word_cooccur_matrix[:100,:100]).all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perform SVD on the matrix and select the largest singular values \n",
    "\n",
    "We perform SVD on the matrix $\\mathbf{M} = \\mathbf{K}\\mathbf{S}\\mathbf{D}^T$ and select the first 128 largest singular values.\n",
    "\n",
    "Then, we can use the submatrix $\\mathbf{K_s}$, corresponding to the largest singular values, as the word representation matrix. \n",
    "\n",
    "Hint 1 : Are the words represented in $\\mathbf{K_s}$ as rows or columns?\n",
    "\n",
    "Hint 2: np.linalg.svd(M, full_matrices=False) performs SVD on the matrix $\\mathbf{M}$ and returns $\\mathbf{K}, \\mathbf{S}, \\mathbf{D}^T$\n",
    "\n",
    " -  $\\mathbf{K}, \\mathbf{D}^T$ are matrices with orthonormal columns\n",
    " -  $\\mathbf{S}$ is a **vector** of singular values in a **descending** order\n",
    " \n",
    "Hint 3: np.diag(V) converts a vector to a diagonal matrix\n",
    "\n",
    "Hint 4: To select:\n",
    " - the first k rows of a matrix A, use A[0:k, :]\n",
    " - the first k columns of a matrix A, use A[:, 0:k]\n",
    " - the submatrix from first k rows and k columns of a matrix A, use A[0:k, 0:k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input: a word coocurrence matrix and the number of singular values that will be selected\n",
    "# Output: K_s, S_s, Dt_s are similar to the defintion in the lecture\n",
    "\n",
    "def truncated_svd(word_cooccur_matrix, num_val):\n",
    "    # The following may take 1-2 minutes since we are decomposing a matrix of size 5000x1075\n",
    "    K, S, Dt = np.linalg.svd(word_cooccur_matrix, full_matrices=False) \n",
    "    \n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    K_sel = K[:, 0:num_val]\n",
    "    S_sel = np.diag(S[0:num_val])\n",
    "    Dt_sel = Dt[0:num_val, :]\n",
    "    return K_sel, S_sel, Dt_sel\n",
    "\n",
    "K_s, S_s, Dt_s = truncated_svd(word_cooccur_matrix,128)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vector-based retrieval using Word representations\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Document and query vectors from word representations\n",
    "\n",
    "For each document and query, we construct the corresponding vector by **averaging** its word representations.\n",
    "\n",
    "Hint: not all words are in the vocabulary!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5000, 128)\n",
      "638857\n",
      "[[ True  True  True ...  True  True  True]\n",
      " [ True  True False ...  True  True  True]\n",
      " [ True False  True ...  True  True  True]\n",
      " ...\n",
      " [ True  True  True ...  True  True  True]\n",
      " [ True  True  True ...  True  True  True]\n",
      " [ True  True  True ...  True  True  True]]\n"
     ]
    }
   ],
   "source": [
    "def get_doc_vecs(documents, word_embedding_matrix, vocabulary_to_index):\n",
    "    doc_vecs = np.zeros((len(documents), word_embedding_matrix.shape[1]))\n",
    "    # YOUR CODE HERE\n",
    "    for i, doc in enumerate(documents):\n",
    "        tmp = []\n",
    "        for word in doc.split():\n",
    "            if word in vocabulary_to_index:\n",
    "                ind = vocabulary_to_index[word]\n",
    "                tmp.append(word_embedding_matrix[ind])\n",
    "        if len(tmp) >= 1:\n",
    "            doc_vecs[i] = np.array(tmp).mean(axis=0)\n",
    "    return doc_vecs\n",
    "\n",
    "# YOUR CODE HERE\n",
    "doc_vecs = get_doc_vecs(documents, K_s, vocabulary_to_index)\n",
    "\n",
    "print(K_s.shape)\n",
    "\n",
    "print(np.array((K_s - Dt_s.T) < 1e-1).sum())\n",
    "\n",
    "print((K_s - Dt_s.T) < 1e-1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieve top-10 relevant documents\n",
    "\n",
    "Retrieve top-10 relevant documents for the query \"*computer science*\"\n",
    "\n",
    "Hint: you may use the function get_doc_vecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"computer science\"\n",
    "\n",
    "# YOUR CODE HERE\n",
    "query_vec = get_doc_vecs([query], K_s, vocabulary_to_index)[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\mao zhipeng\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\ipykernel_launcher.py:8: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Exposure Science Film Hackathon 2017 applications open! Come join our Scicomm-film-hacking event! #Science #scicomm https://t.co/zwtKPlh6HT\\n',\n",
       " 'Exciting News: \"World University Rankings 2016-2017 by subject: computer science\" No1 @ETH &amp; @EPFL on No8. Congrats https://t.co/ARSlXZoShQ\\n',\n",
       " 'New report on risk of misuse of life science research https://t.co/6HaoSkJTmZ #epfl  @ScnatCH https://t.co/VmkwW0Q2Gq\\n',\n",
       " \"#sleep #neuroscience #Neurosciences #science Healthy mitochondria could stop Alzheimer's https://t.co/rioZv4axxN #epfl\\n\",\n",
       " '@MartinVetterli @EPFL setzt sich für open science ein. Wissen teilen für mehr Wirkung. https://t.co/4hMAZtA9Qa\\n',\n",
       " '\"A parametric tool to evaluate the environmental and economic feasibility of decentralized energy systems.\" - https://t.co/mC2D0L1xRW\\n',\n",
       " \"Fantastic multimedia presentation on solving paralysis by @EPFL_en 's Gregoire Courtine. Great to see 1st human steps #wysssupport #INS2017\\n\",\n",
       " \"Time for Open Farm's pitch #openfood #Opendata #EPFL https://t.co/7lKKao5780\\n\",\n",
       " \"Très bel exemple d'une collaboration interdisciplinaire : quand les sciences humaines ont recours à la physique https://t.co/EHgKcHYUD6 @EPFL @frederickaplan #archivCH #archimag\\n\",\n",
       " 'It’s possible to film quantum mechanics – and its paradoxical nature – directly. Here is light photographed as a Wave &amp; as a Particle for the first time via @EPFL_en @EPFL https://t.co/m3h6iyAV3K\\n']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def cosine_similarity(v1, v2):\n",
    "    sumxx, sumxy, sumyy = 0, 0, 0\n",
    "    for i in range(len(v1)):\n",
    "        x = v1[i]; y = v2[i]\n",
    "        sumxx += x*x\n",
    "        sumyy += y*y\n",
    "        sumxy += x*y\n",
    "    return sumxy*1.0/math.sqrt(sumxx*sumyy)\n",
    "\n",
    "def retrieve_documents(doc_vecs, query_vec, top_k):\n",
    "    scores = [[cosine_similarity(query_vec, doc_vecs[d,:]), d] for d in range(len(documents))]\n",
    "    scores.sort(key=lambda x: -x[0])\n",
    "    doc_ids = []\n",
    "    retrieved = []\n",
    "    for i in range(top_k):\n",
    "        doc_ids.append(scores[i][1])\n",
    "        retrieved.append(orig_docs[scores[i][1]])\n",
    "    return doc_ids, retrieved\n",
    "\n",
    "retrieved_ids, retrieved_docs = retrieve_documents(doc_vecs, query_vec, top_k=10)\n",
    "\n",
    "retrieved_docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation\n",
    "We consider the scikit reference code as an “oracle” that supposedly gives the correct result. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['exciting news world university rankings subject computer science eth epfl congrats httpstcoarslxzoshq',\n",
       " 'new computer model shows proteins controlled distance via epflen vdtech',\n",
       " 'interview patrick barth new epfl professor combines protein biophysics computer modeling httpstcoijwbaebocj',\n",
       " 'exposure science film hackathon applications open come join scicommfilmhacking event science scicomm',\n",
       " 'rêve science linnovation touchées décret moyenage trump nobannowall epfl tdgch',\n",
       " 'mystère soulages éblouit science epfl',\n",
       " 'internet amplifying popularity irrational food fads time science says epflen',\n",
       " 'interessanter artikel von mirkobischberg epfl aus texas virtualreality science arts evolution der maschinen',\n",
       " 'blue brain nexus opensource tool datadriven science epfl',\n",
       " 'blue brain nexus opensource tool datadriven science via epflen vdtech',\n",
       " 'cwarwarrior epflen epfl science epflen indeed pretty cool thank visiting',\n",
       " 'swiss data science twitter sign epflen datajamdays learn httpstcoknvilhwpgb see',\n",
       " 'registration exposure science film hackathon open scicomm lausanne epfl unil',\n",
       " 'energy people blue brain nexus opensource tool datadriven science epfl',\n",
       " 'new report risk misuse life science research epfl scnatch',\n",
       " 'know someone promoted sound science less weeks nominate maddoxprize',\n",
       " 'sleep neuroscience neurosciences science healthy mitochondria could stop alzheimers epfl',\n",
       " 'deep learning graphs christmas lecture epflen course network tour data science',\n",
       " 'data science mobility conference coorganized epfl sbb cff ffs janv epfl',\n",
       " 'today visited friend wonderqueens epflen showed around unbelievably cool place science thanks',\n",
       " 'hey trainees looking postdoc genomics come switzerland lake mountains exciting science',\n",
       " 'blue brain nexus opensource knowledge graph datadriven science via epflen eurekalert bluebrainpjt wakesleep',\n",
       " 'eth zürich und epfl haben das swiss data science center eröffnet eth epflen sdscdatascience',\n",
       " 'noir cest noir outrenoirs pierre soulages culture art science epfl epflcampus',\n",
       " 'martinvetterli epfl setzt sich für open science ein wissen teilen für mehr wirkung',\n",
       " 'open call science technology womenentrepreneurs applications musy award epfl vp innovation open apply get chance win chf find',\n",
       " 'art science collisions presentation art residence cern yunchul kim helga timko epfl']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Retrieval oracle \n",
    "tf = TfidfVectorizer(analyzer='word', ngram_range=(1,1), vocabulary=vocab_freq, min_df = 1, stop_words = 'english')\n",
    "features = tf.fit_transform(documents)\n",
    "npm_tfidf = features.todense()\n",
    "\n",
    "# Return all document ids that that have cosine similarity with the query larger than a threshold\n",
    "def search_vec_sklearn(query, features, threshold=0.1):\n",
    "    new_features = tf.transform([query])\n",
    "    cosine_similarities = linear_kernel(new_features, features).flatten()\n",
    "    related_docs_indices, cos_sim_sorted = zip(*sorted(enumerate(cosine_similarities), key=itemgetter(1), \n",
    "                                                       reverse=True))\n",
    "    doc_ids = []\n",
    "    for i, cos_sim in enumerate(cos_sim_sorted):\n",
    "        if cos_sim < threshold:\n",
    "            break\n",
    "        doc_ids.append(related_docs_indices[i])\n",
    "    return doc_ids\n",
    "\n",
    "# gt_ids are the document ids retrieved by the oracle\n",
    "gt_ids = search_vec_sklearn(query, features)\n",
    "\n",
    "[documents[i] for i in gt_ids]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also assume that there is a user that has done the grading of all the documents according to their relevance. \n",
    "The top-10 results using scikit-learn have grade 3, the next 10 results have grade 2, \n",
    "the rest in the list has grade 1 while non-relevant results have grade 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "grade = []\n",
    "for i in range(len(documents)):\n",
    "    if i in gt_ids[:10]:\n",
    "        grade.append(3)\n",
    "    elif i in gt_ids[10:20]:\n",
    "        grade.append(2)\n",
    "    elif i in gt_ids[20:]:\n",
    "        grade.append(1)\n",
    "    else:\n",
    "        grade.append(0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate retrieval result using DCG \n",
    "\n",
    "Discounted Cumulative Gain (DCG) is a retrieval metric that also takes into account the ordering of the result. \n",
    "\n",
    "The DCG accumulated at a rank $k$ is defined as:\n",
    "\n",
    "$DCG_k = \\sum_{i=1}^k \\frac{grade[i]}{log_2(i+1)}$\n",
    "\n",
    "where $grade[i]$ is the relevance score given by the user for the result at position $i$.\n",
    "\n",
    "Hint: the logarithm is computed using the function np.log2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3, 3, 2, 2, 1, 0, 0, 0, 0, 0]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def dcg(k, retrieved_ids, grade):\n",
    "    dcg_val = np.array([grade[retrieved_ids[i]] / np.log2(i + 2) for i in range(0, k)]).sum()\n",
    "    return dcg_val\n",
    "\n",
    "[grade[i] for i in retrieved_ids]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We compute the DCG for the top-1 to the top-10 retrieval results and we plot the values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x2598f371088>]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "vals = []\n",
    "for i in range(10):\n",
    "    val = dcg(i, retrieved_ids, grade)\n",
    "    vals.append(val)\n",
    "    \n",
    "plt.plot(vals)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explain the DCG values plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submit your notebook\n",
    "\n",
    "Go to [Moodle](https://moodle.epfl.ch/course/view.php?id=4051) > Exams > Midterm and follow the instructions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "toc": {
   "base_numbering": "0",
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "400px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
